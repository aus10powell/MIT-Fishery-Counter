{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Vision\n",
    "import cv2\n",
    "\n",
    "## Utility\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "import inference_utils\n",
    "\n",
    "## Displaying in-cell\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Analysis\n",
    "from collections import Counter\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_DIR = os.path.join(\"/Users\", \"apowell\", \"Downloads\")\n",
    "assert os.path.isdir(DOWNLOAD_DIR)\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"Herring\")\n",
    "\n",
    "TRAINING_DIR = os.path.join(DATA_DIR, \"Training\", \"IRWA 2017 (Large+annotations)\")\n",
    "TESTING_DIR = os.path.join(DATA_DIR, \"Testing\")\n",
    "\n",
    "os.path.isdir(DATA_DIR)  # assertion that we are pointed to correct folder\n",
    "WEIGHTS_DIR = os.path.join(\"..\", \"weights_and_config\")\n",
    "HERRING_WEIGHTS = os.path.join(WEIGHTS_DIR, \"Herring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FishCounter(object):\n",
    "    import cv2\n",
    "\n",
    "    def __init__(self, model_config, model_weights):\n",
    "        \"\"\"model_config and model_weights are paths.\"\"\"\n",
    "        self.net = cv2.dnn.readNetFromDarknet(model_config, model_weights)\n",
    "        self.input_width = 416\n",
    "        self.input_height = 416\n",
    "        self.conf_threshold = 0.5  # Confidence threshold\n",
    "        self.nms_threshold = (\n",
    "            0.05  # Non-maximum suppression threshold (maximum bounding box)\n",
    "        )\n",
    "\n",
    "    def inference(self, image):\n",
    "        \"\"\"Create a 4D blob from a single frame.\n",
    "        Args:\n",
    "            - frame: path to jpg image\n",
    "        \"\"\"\n",
    "        self.frame = cv2.imread(image)\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            frame,\n",
    "            1 / 255,\n",
    "            (self.input_width, self.input_height),\n",
    "            [0, 0, 0],\n",
    "            1,\n",
    "            crop=False,\n",
    "        )\n",
    "\n",
    "        # Sets the input to the network\n",
    "        self.net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        self.outs = self.net.forward(self._get_outputs_names(net=self.net))\n",
    "\n",
    "    def _get_outputs_names(self, net):\n",
    "        \"\"\"Get the names of the output layers.\"\"\"\n",
    "        # Get the names of all the layers in the network\n",
    "        layersNames = self.net.getLayerNames()\n",
    "        # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
    "        return [layersNames[i - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "\n",
    "    def process_frame(self):\n",
    "        frame_height = self.frame.shape[0]\n",
    "        frame_width = self.frame.shape[1]\n",
    "\n",
    "        # Scan through all the bounding boxes output from the network and keep only the\n",
    "        # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
    "        class_ids, confidences, boxes = [], [], []\n",
    "        counts = 0\n",
    "        for out in self.outs:  # Scan through bounding boxes\n",
    "            for detection in out:  # Scan through\n",
    "                scores = detection[5:]\n",
    "\n",
    "                class_id = np.argmax(scores)  # class with highest score\n",
    "                confidence = scores[class_id]  # confidence score for class\n",
    "                if confidence > conf_threshold:\n",
    "                    print(detection)\n",
    "                    center_x = int(detection[0] * frame_width)\n",
    "                    center_y = int(detection[1] * frame_height)\n",
    "                    width = int(detection[2] * frame_width)\n",
    "                    height = int(detection[3] * frame_height)\n",
    "                    left = int(center_x - width / 2)\n",
    "                    top = int(center_y - height / 2)\n",
    "                    class_ids.append(class_id)\n",
    "                    confidences.append(float(confidence))\n",
    "                    boxes.append([left, top, width, height])\n",
    "                    counts += 1\n",
    "\n",
    "        # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "        # lower confidences.\n",
    "        indices = cv.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "        print(\"BOXES\", boxes, indices, class_ids)\n",
    "\n",
    "    def get_annotated_frame(self):\n",
    "        \"\"\"Returns main frame with fish highlighted.\"\"\"\n",
    "        frame = self.frame.copy()\n",
    "        pass  # TODO: Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36366087 0.6363174  0.41520736 0.20833406 0.9303912  0.9303912 ]\n",
      "[0.9431358  0.6381941  0.10756955 0.11566652 0.8135409  0.8135409 ]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m c \u001b[39m=\u001b[39m FishCounter(model_config\u001b[39m=\u001b[39mmodel_config, model_weights\u001b[39m=\u001b[39mmodel_weights)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m c\u001b[39m.\u001b[39minference(image\u001b[39m=\u001b[39mherring_image)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m c\u001b[39m.\u001b[39;49mprocess_frame()\n",
      "\u001b[1;32m/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb Cell 5\u001b[0m in \u001b[0;36mFishCounter.process_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb#X45sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m             counts \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb#X45sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# Perform non maximum suppression to eliminate redundant overlapping boxes with\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb#X45sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# lower confidences.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb#X45sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m indices \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mdnn\u001b[39m.\u001b[39mNMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apowell/Documents/Projects/MIT-Fishery-Counter/code/inference_analysis.ipynb#X45sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBOXES\u001b[39m\u001b[39m\"\u001b[39m, boxes, indices, class_ids)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "model_config = os.path.join(HERRING_WEIGHTS, \"herring.cfg\")\n",
    "model_weights = os.path.join(HERRING_WEIGHTS, \"herring_final.weights\")\n",
    "c = FishCounter(model_config=model_config, model_weights=model_weights)\n",
    "c.inference(image=herring_image)\n",
    "c.process_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimensions_of_annotations(fpath):\n",
    "    \"\"\"Extracts dimensions of annotation boxes from xml file for image.\"\"\"\n",
    "\n",
    "    class_label = \"herring\"  # TODO: Will need to accomodate multiple classes\n",
    "\n",
    "    tree = ET.parse(fpath)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    xmins = []\n",
    "    ymins = []\n",
    "    xmaxs = []\n",
    "    ymaxs = []\n",
    "    for o in root.iter(\"object\"):\n",
    "        if o.find(\"name\").text == class_label:\n",
    "            xmins.append(eval(o[4][0].text))\n",
    "            ymins.append(eval(o[4][1].text))\n",
    "            xmaxs.append(eval(o[4][2].text))\n",
    "            ymaxs.append(eval(o[4][3].text))\n",
    "    return xmins, ymins, xmaxs, ymaxs\n",
    "\n",
    "\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    \"\"\"Compute the intersection over union by taking the intersection area and dividing\n",
    "    it by the sum of prediction + ground-truth areas - the interesection area\"\"\"\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    print(interArea)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    print(boxAArea)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    print(boxBArea)\n",
    "\n",
    "    return interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "\n",
    "def find_matching_path(annotations, images):\n",
    "    \"\"\"Finds the corresponding image or annotation path (i.e. given on path searches for the corresponding path)\"\"\"\n",
    "    if type(annotations) == str:\n",
    "        anchor, search = annotations, images\n",
    "    elif type(images) == str:\n",
    "        anchor, search = images, annotations\n",
    "    else:\n",
    "        print(\"Somethings wrong\")\n",
    "    return [p for p in search if anchor.split(\"/\")[-1].split(\".\")[0] in p][\n",
    "        0\n",
    "    ]  # only need one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 images. Annotations: 94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "folder = \"Batch 1\"\n",
    "# Load images and annotations\n",
    "# images_path1 = os.path.join(TRAINING_DIR, folder, \"*.JPG\")\n",
    "# images_path2 = os.path.join(TRAINING_DIR, folder, \"*.jpg\")\n",
    "images_path1 = os.path.join(DOWNLOAD_DIR, \"HerringTestSet\", \"Images\", \"*.jpg\")\n",
    "images = glob.glob(images_path1)  # + glob.glob(images_path2)\n",
    "\n",
    "# annotation_path = os.path.join(TRAINING_DIR, f\"{folder} Annotations\", \"*.xml\")\n",
    "annotation_path = os.path.join(DOWNLOAD_DIR, \"HerringTestSet\", \"Xml\", \"*.xml\")\n",
    "annotations = glob.glob(annotation_path)\n",
    "print(\"{} images. Annotations: {}\".format(len(images), len(annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: /Users/apowell/Downloads/HerringTestSet/Images/1_2016-04-21_21-50-1808716.jpg\n",
      "test match: /Users/apowell/Downloads/HerringTestSet/Xml/1_2016-04-21_21-50-1808716.xml\n"
     ]
    }
   ],
   "source": [
    "# Example: Finding matching paths\n",
    "test = images[0]\n",
    "print(\"test:\", test)\n",
    "test_match = find_matching_path(images=test, annotations=annotations)\n",
    "print(\"test match:\", test_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring annotations against inference\n",
    "**Requirements:** 2 folders: 1 with annotations (e.g. XML), 1 with images\n",
    "* **Steps:**\n",
    "  * 1) Run loop on images extracting predicted boxes from each image\n",
    "  * 2) Scan filenames until box is properly extracted\n",
    "  * 3) **Scoring Method:**\n",
    "    * For each annotation find closest/any overlapping boxes (scoring the greater of number of either the annotated or predicted set distinctly)\n",
    "    * Return average IOU for image\n",
    "    * Return total number of correctly detected fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_comparison(img_path=None, annotated_box=None, predicted_box=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cv2\n",
    "\n",
    "    \"\"\"Display an image with optional annotated class and predicted class within notebook for comparison.\n",
    "    image_path: image path to location in memory\n",
    "    annotated_box: corners in format xmin,xmax,ymin,ymax\n",
    "    \"\"\"\n",
    "\n",
    "    label = \"herring\"\n",
    "\n",
    "    img_name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    frame = cv2.imread(img_path)\n",
    "\n",
    "    # Annotated\n",
    "    cv2.rectangle(\n",
    "        frame,\n",
    "        (annotated_box[0], annotated_box[1]),\n",
    "        (annotated_box[2], annotated_box[3]),\n",
    "        (178, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    # Annotated\n",
    "    # Predicted\n",
    "    if predicted_box is not None:\n",
    "        cv2.rectangle(\n",
    "            frame,\n",
    "            (predicted_box[0], predicted_box[1]),\n",
    "            (predicted_box[2], predicted_box[3]),\n",
    "            (144, 238, 144),\n",
    "            3,\n",
    "        )\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        \"Annotated \" + label,\n",
    "        (annotated_box[0], annotated_box[1]),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.55,\n",
    "        (178, 255, 255),\n",
    "        1,\n",
    "    )\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        \"Predicted \" + label,\n",
    "        (annotated_box[1], annotated_box[2]),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.55,\n",
    "        (144, 238, 144),\n",
    "        1,\n",
    "    )\n",
    "    plt.imshow(frame)\n",
    "    plt.title(img_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Example files\n",
    "# 1 count: ../data/Herring/Training/IRWA 2017 (Large+annotations)/Batch 1/2_2017-04-14_11-42-13_large.jpg\n",
    "# herring_image = \"../data/Herring/Training/IRWA 2017 (Large+annotations)/Batch 1/2_2017-04-14_11-42-13_large.jpg\"\n",
    "# boxes = [[56, 134, 97, 48]]\n",
    "\n",
    "# # 1 count: ../data/Herring/Training/IRWA 2017 (Large+annotations)/Batch 1/2_2017-04-14_10-51-06_large.jpg\n",
    "# herring_image = \"../data/Herring/Training/IRWA 2017 (Large+annotations)/Batch 1/2_2017-04-14_10-51-06_large.jpg\"\n",
    "# boxes = [[6, 110, 107, 56]]\n",
    "\n",
    "# print(herring_image, boxes)\n",
    "# # Get index of annotation path files that contains the path to annotated file matching the non-annoated file\n",
    "# idx = [\n",
    "#     i\n",
    "#     for i in range(len(annotations))\n",
    "#     if herring_image.split(\"/\")[-1].split(\".\")[0] in annotations[i]\n",
    "# ][\n",
    "#     0\n",
    "# ]  # Get the first element (multiple matches do not matter)\n",
    "\n",
    "\n",
    "# xmins, xmaxs, ymins, ymaxs = get_dimensions_of_annotations(fpath=annotations[idx])\n",
    "\n",
    "# # Predicted\n",
    "# left, top, width, height = boxes[0][0], boxes[0][1], boxes[0][2], boxes[0][3]\n",
    "# xmin, ymin, xmax, ymax = left, top, left + width, top + height\n",
    "# predicted_box = xmin, ymin, xmax, ymax\n",
    "# # Annotated\n",
    "# annotated_box = xmins[0], xmaxs[0], ymins[0], ymaxs[0]\n",
    "# print(\"IOU Score:\", bb_intersection_over_union(boxA=predicted_box, boxB=annotated_box))\n",
    "\n",
    "# display_image_comparison(\n",
    "#     img_path=herring_image, annotated_box=annotated_box, predicted_box=predicted_box\n",
    "# )  # predicted_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([68], [229], [396], [367])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the annotation file\n",
    "annotation_file = find_matching_path(\n",
    "    images=herring_image.split(\"/\")[-1].split(\".\")[0], annotations=annotations\n",
    ")\n",
    "get_dimensions_of_annotations(fpath=a)\n",
    "# xmins, xmaxs, ymins, ymaxs = get_dimensions_of_annotations(fpath=a)\n",
    "# annotated_box = xmins[0], xmaxs[0], ymins[0], ymaxs[0]\n",
    "\n",
    "# display_image_comparison(\n",
    "#     img_path=herring_image, annotated_box=annotated_box, predicted_box=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(xmins): 2\n"
     ]
    }
   ],
   "source": [
    "## Initialize the parameters\n",
    "# Confidence threshold\n",
    "conf_threshold = 0.5\n",
    "# Non-maximum suppression threshold (maximum bounding box)\n",
    "nms_threshold = 0.05\n",
    "input_width = 416  # Width of network's input image\n",
    "input_height = 416  # Height of network's input image\n",
    "\n",
    "# Load class name\n",
    "classes = \"Herring\"\n",
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "modelConfiguration = os.path.join(HERRING_WEIGHTS, \"herring.cfg\")\n",
    "modelWeights = os.path.join(HERRING_WEIGHTS, \"herring_final.weights\")\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "\n",
    "\n",
    "iou_scores = []\n",
    "annotated_counts = []\n",
    "predicted_counts = []\n",
    "# Loop through annotated files\n",
    "i, n = 0, len(annotations)\n",
    "\n",
    "for a in annotations:\n",
    "    print(f\"{i} of {n}\")\n",
    "    name = a.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # Loop through image files (non-annotated) to make a prediction on\n",
    "    for herring_image in images:\n",
    "\n",
    "        if name not in herring_image:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.imread(herring_image)\n",
    "\n",
    "        # # Get frame from the video\n",
    "        # hasFrame, frame = cap.read()\n",
    "\n",
    "        input_width, input_height, _ = frame.shape\n",
    "        input_width = input_height = min(input_width, input_height)\n",
    "        input_width = 416  # Width of network's input image\n",
    "        input_height = 416  # Height of network's input image\n",
    "\n",
    "        print(input_height, input_width)\n",
    "        # Create a 4D blob from a frame.\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            frame, 1 / 255, (input_width, input_height), [0, 0, 0], 1, crop=False\n",
    "        )\n",
    "\n",
    "        # Sets the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        outs = net.forward(inference_utils.get_outputs_names(net=net))\n",
    "        # break\n",
    "        # Remove the bounding boxes with low confidence\n",
    "        counts, boxes = inference_utils.postprocess(\n",
    "            frame=frame,\n",
    "            outs=outs,\n",
    "            tracker=None,\n",
    "            conf_threshold=conf_threshold,\n",
    "            nms_threshold=nms_threshold,\n",
    "            classes=classes,\n",
    "        )\n",
    "\n",
    "        print(boxes)\n",
    "        # cv2.imshow(herring_image, frame)\n",
    "        # cv2.waitKey(0)\n",
    "        print(\"Counts:\", counts)\n",
    "\n",
    "        # Get score\n",
    "        xmins, xmaxs, ymins, ymaxs = get_dimensions_of_annotations(fpath=a)\n",
    "        # Predicted\n",
    "        if len(boxes) == 0:  # If nothing was predicted\n",
    "            boxes = [[0, 0, 0, 0]]\n",
    "        left, top, width, height = boxes[0][0], boxes[0][1], boxes[0][2], boxes[0][3]\n",
    "        xmin, ymin, xmax, ymax = left, top, left + width, top + height\n",
    "        predicted_box = xmin, ymin, xmax, ymax\n",
    "        # Annotated\n",
    "        if len(xmins) == 0:  # If nothing was annotated\n",
    "            annotated_box = [0, 0, 0, 0]\n",
    "            annotated_count = 0\n",
    "        else:\n",
    "            annotated_box = xmins[0], xmaxs[0], ymins[0], ymaxs[0]\n",
    "            annotated_count = len(xmins)\n",
    "\n",
    "        annotated_counts.append(annotated_count)\n",
    "        predicted_counts.append(counts)\n",
    "        iou_score = bb_intersection_over_union(boxA=predicted_box, boxB=annotated_box)\n",
    "        iou_scores.append(iou_score)\n",
    "        print(\"IOU Score:\", iou_score)\n",
    "        print()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average IOU: 0.805932661840877\n",
      "Total annotated counts: 112\n",
      "Annotated Counts breakdown: Counter({1: 77, 2: 16, 3: 1}) \n",
      "CI: (1.0605317892751602, 1.322446934129095)\n",
      "\n",
      "Total predicted counts: 105\n",
      "Annotated Counts breakdown: Counter({1: 83, 2: 11})\n",
      "Ratio of predicted to expected: 0.938\n",
      "CI: (1.0024051012890367, 1.2316374519024524)\n"
     ]
    }
   ],
   "source": [
    "def counts_ci(x):\n",
    "    \"\"\"Confidence interval for counts.\"\"\"\n",
    "    m = np.mean(x)\n",
    "    me = 1.96 * np.sqrt(np.std(x) / len(x))\n",
    "    return m - me, m + me\n",
    "\n",
    "\n",
    "print(\"average IOU:\", np.mean(iou_scores))\n",
    "print(\"Total annotated counts:\", sum(annotated_counts))\n",
    "print(\n",
    "    f\"Annotated Counts breakdown: {Counter(annotated_counts)} \",\n",
    ")\n",
    "print(f\"CI: {counts_ci(annotated_counts)}\")\n",
    "print()\n",
    "print(\"Total predicted counts:\", sum(predicted_counts))\n",
    "print(\"Annotated Counts breakdown:\", Counter(predicted_counts))\n",
    "print(\n",
    "    f\"Ratio of predicted to expected: { sum(predicted_counts) / sum(annotated_counts):.3f}\"\n",
    ")\n",
    "print(f\"CI: {counts_ci(predicted_counts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
