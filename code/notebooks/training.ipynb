{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training YOLO with Ultralytics pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcolabcode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColabCode\n\u001b[0;32m----> 2\u001b[0m ColabCode(port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-env/lib/python3.11/site-packages/colabcode/code.py:42\u001b[0m, in \u001b[0;36mColabCode.__init__\u001b[0;34m(self, port, password, authtoken, mount_drive, code, lab)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_lab()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_install_code()\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_install_extensions()\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_server()\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-env/lib/python3.11/site-packages/colabcode/code.py:49\u001b[0m, in \u001b[0;36mColabCode._install_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_install_code\u001b[39m():\n\u001b[0;32m---> 49\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwget\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://code-server.dev/install.sh\u001b[39m\u001b[38;5;124m\"\u001b[39m], stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[1;32m     50\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m     51\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall.sh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--version\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCODESERVER_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     52\u001b[0m         stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m     53\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-env/lib/python3.11/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-env/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m~/anaconda3/envs/vision-env/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wget'"
     ]
    }
   ],
   "source": [
    "from colabcode import ColabCode\n",
    "ColabCode(port=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.147 ðŸš€ Python-3.11.3 torch-2.1.0.dev20230515 CPU (Apple M2 Pro)\n",
      "Setup complete âœ… (10 CPUs, 32.0 GB RAM, 485.9/926.4 GB disk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab_black'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39m# assert sys.version_info >= (3, 8), \"Python 3.8  is required.\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlab_black\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2414\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2412\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2413\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2414\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   2416\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2417\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2418\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/IPython/core/magics/extension.py:33\u001b[0m, in \u001b[0;36mExtensionMagics.load_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m module_str:\n\u001b[1;32m     32\u001b[0m     \u001b[39mraise\u001b[39;00m UsageError(\u001b[39m'\u001b[39m\u001b[39mMissing module name.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshell\u001b[39m.\u001b[39mextension_manager\u001b[39m.\u001b[39mload_extension(module_str)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39malready loaded\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m extension is already loaded. To reload it, use:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m module_str)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/IPython/core/extensions.py:76\u001b[0m, in \u001b[0;36mExtensionManager.load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[39mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_extension(module_str)\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m module_str \u001b[39min\u001b[39;00m BUILTINS_EXTS:\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/IPython/core/extensions.py:91\u001b[0m, in \u001b[0;36mExtensionManager._load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshell\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m module_str \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[0;32m---> 91\u001b[0m         mod \u001b[39m=\u001b[39m import_module(module_str)\n\u001b[1;32m     92\u001b[0m     mod \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[module_str]\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_load_ipython_extension(mod):\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab_black'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os, sys\n",
    "from tqdm.notebook import tqdm\n",
    "import ultralytics\n",
    "\n",
    "ultralytics.checks()\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# assert sys.version_info >= (3, 8), \"Python 3.8  is required.\"\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "#model = YOLO('yolov8n.yaml')  # build a new model from scratch\n",
    "model = YOLO('yolov8m.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "# # If using MPS on Apple Silicon\n",
    "# if torch.backends.mps.is_built():\n",
    "#     mps_device = torch.device(\"mps\")\n",
    "#     model.to(mps_device)\n",
    "\n",
    "# # OPTIONAL for memory errors\n",
    "# # Set the value of PYTORCH_MPS_HIGH_WATERMARK_RATIO\n",
    "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] =  \"0.3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"MITFisheryHerring\",notes=\"MIT Fishery robo analytics Outputs per training example: 2 Blur: Up to 4px. Now added ~50 more images. Also training on yolov8m vs yolov8n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model\n",
    "results = model.train(data='river_herring.yaml', epochs=15,dropout=0.4,device='mps')  # can also try mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics.yolo.utils.benchmarks import benchmark\n",
    "\n",
    "# # Benchmark on GPU\n",
    "# benchmark(model=model, imgsz=640, half=False, device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "model.export(format='saved_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Record of models:\n",
    "## \n",
    "# 'runs/detect/train36/weights/best_saved_model' Wandb:  https://wandb.ai/aus10powell/MITFisheryHerring/runs/txn0c9x3\n",
    "from ultralytics import YOLO\n",
    "model_best = YOLO( \"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/runs/detect/train196/weights/best.pt\") \n",
    "model_best = YOLO(\"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/runs/detect/train30/weights/best.pt\")\n",
    "model_best = YOLO(\"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/runs/detect/train79/weights/last.pt\")\n",
    "model_best = YOLO(\"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/runs/colab_runs/best7.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model_best.val(split='val'))\n",
    "print(\"*\"*50)\n",
    "display(model_best.val(split='test',device='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "herring_example = '/Users/aus10powell/Downloads/RiverHerring/River Herring JRWA/2013 Johne RIver Wapping Rd Dam/vlcsnap-2016-12-20-11h40m56s911.jpg'\n",
    "white_sucker_example = '/Users/aus10powell/Downloads/RiverHerring/River Herring JRWA/2013 Underwater fish Jones River/WhiteSuckerWapping (6).JPG'\n",
    "small_image = '/Users/aus10powell/Downloads/RiverHerring/IRWA 2017 (Large+annotations)/IRWA Small Jpgs/Batch 1/2_2017-04-14_19-03-04.jpg'\n",
    "v2_test_image = '/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/data/datasets/MITFisheryHerringv2/test/images/1_2016-04-21_21-50-1804298_jpg.rf.011911b9c681d5b4af70a308cdd98b93.jpg' # 2 fish\n",
    "v2_valid_image = '/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/data/datasets/MITFisheryHerringv2/valid/images/1_2016-04-21_21-50-1803927_jpg.rf.6b36f83367e966f41544ba7e229f4d05.jpg'\n",
    "\n",
    "random_example = \"/Users/aus10powell/Downloads/RiverHerring/River Herring IRWA/1_2016-05-13_12-34-57_large.jpg\"\n",
    "\n",
    "#\n",
    "v2_valid_imageb = \"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/data/datasets/MITRiverHerring2w/valid/images/1_2016-04-21_21-50-1804987_jpg.rf.bd983d16bbaf541a0f08db030018a540.jpg\"\n",
    "v2_valid_imagec = \"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/data/datasets/MITRiverHerring2w/valid/images/1_2016-04-21_21-50-1805011_jpg.rf.27d6d6392af4e7f3f8cac5c5bde0c0d1.jpg\"\n",
    "\n",
    "image = \"/Users/aus10powell/Downloads/RiverHerring/River Herring IRWA/1_2016-04-23_16-06-05_large.jpg\"\n",
    "\n",
    "results = model_best(image)\n",
    "title = image.split(\"/\")[-1]\n",
    "num_fish = len(results[0].boxes.data)\n",
    "\n",
    "# Load and display the original image\n",
    "original_image = cv2.imread(image)\n",
    "original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "# Generate and display the annotated frame\n",
    "annotated_frame = results[0].plot()\n",
    "gray = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2GRAY)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(gray, cmap=\"gray\")\n",
    "plt.title(title + f' ---- num_fish: {num_fish}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images with biggest loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# model = model_best\n",
    "# video_path = \"/Users/aus10powell/Downloads/RiverHerring/IRWA 2017 Videos/Fish Sightings 2017/2_2017-04-13_14-10-29.mp4\"\n",
    "# results = model.track(\n",
    "#     source=video_path,\n",
    "#     tracker=\"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/bytetrack.yaml\",\n",
    "#     show=True,\n",
    "#    #stream=True #Uncomment to use r in results as loop\n",
    "# )  # OPTIONAL: , tracker='custom_tracker.yaml'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Directory path containing the JPEG images\n",
    "directory = '/Users/aus10powell/Downloads/RiverHerring/IRWA 2017 Videos/2018 Fish Sightings/2_2018-04-14_09-57-26.mp4'\n",
    "\n",
    "# Initialize variables\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "# Iterate through the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.jpg'):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(directory, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Get the dimensions\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        # Append dimensions to the lists\n",
    "        widths.append(width)\n",
    "        heights.append(height)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "widths = np.array(widths)\n",
    "heights = np.array(heights)\n",
    "\n",
    "# Calculate statistics\n",
    "max_width = np.max(widths)\n",
    "max_height = np.max(heights)\n",
    "min_width = np.min(widths)\n",
    "min_height = np.min(heights)\n",
    "avg_width = np.mean(widths)\n",
    "avg_height = np.mean(heights)\n",
    "std_width = np.std(widths)\n",
    "std_height = np.std(heights)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum Width: {max_width}px\")\n",
    "print(f\"Maximum Height: {max_height}px\")\n",
    "print(f\"Minimum Width: {min_width}px\")\n",
    "print(f\"Minimum Height: {min_height}px\")\n",
    "print(f\"Average Width: {avg_width}px\")\n",
    "print(f\"Average Height: {avg_height}px\")\n",
    "print(f\"Standard Deviation of Widths: {std_width:.2f}px\")\n",
    "print(f\"Standard Deviation of Heights: {std_height:.2f}px\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get frame size of single video\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "def get_frame_size(mp4_file):\n",
    "    cap = cv2.VideoCapture(mp4_file)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Error opening video file: {mp4_file}\")\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return (frame_width, frame_height)\n",
    "\n",
    "# Example usage\n",
    "mp4_file = '/Users/aus10powell/Downloads/RiverHerring/IRWA 2017 Videos/2018 Fish Sightings/2_2018-04-14_09-57-26.mp4'\n",
    "frame_width, frame_height = get_frame_size(mp4_file)\n",
    "\n",
    "print(f\"Frame width: {frame_width}\")\n",
    "print(f\"Frame height: {frame_height}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess videos in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "%config Application.log_level='DEBUG'\n",
    "\n",
    "import os, glob\n",
    "from tqdm import tqdm\n",
    "from main_inference import main\n",
    "video_folder_path =\"/Users/aus10powell/Downloads/RiverHerring/IRWA 2017 Videos/Fish Sightings 2017\"\n",
    "\n",
    "mp4_files = glob.glob(video_folder_path + \"/*.mp4\")\n",
    "\n",
    "print(\"Num .mp4 files:\",len(mp4_files))\n",
    "\n",
    "pred_net_counts = []\n",
    "\n",
    "total_duration_seconds = []\n",
    "\n",
    "for idx in range(len(mp4_files)):\n",
    "    print(f\"Num {idx+1} of {len(mp4_files)}\")\n",
    "    video_path = mp4_files[idx]\n",
    "    # Get counts\n",
    "    frame_rate, annotated_frames, out_count, in_count,duration_seconds,_ = main(\n",
    "        video_path=video_path,device='mps',stream=True,show=False      \n",
    "    )\n",
    "    pred_net_counts.append(out_count-in_count)\n",
    "    total_duration_seconds.append(duration_seconds)\n",
    "print('pred net counts:',pred_net_counts)\n",
    "print(f'total video minutes: {total_duration_seconds/60:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pred net counts:',pred_net_counts)\n",
    "print(f'total video minutes: {total_duration_seconds/60:.1f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Gold Standard Count Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# import glob\n",
    "# import os\n",
    "# PROJECT_DIR = os.path.join(\n",
    "#     \"/\", \"Users\", \"aus10powell\", \"Documents\", \"Projects\", \"MIT-Fishery-Counter\"\n",
    "# )\n",
    "# DATA_DIR = os.path.join(PROJECT_DIR, \"data\")\n",
    "# GOLD_DIR = os.path.join(DATA_DIR, \"gold_dataset\")\n",
    "# mp4_files = glob.glob(os.path.join(GOLD_DIR, \"videos\") + \"/*.mp4\")\n",
    "# #[f.split(\"/\")[-1].split(\".\")[0] for f in mp4_files]\n",
    "\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "# from main_inference import main\n",
    "\n",
    "# import cv2\n",
    "# import time\n",
    "# from ultralytics import YOLO\n",
    "# import supervision as sv\n",
    "# import numpy as np\n",
    "# import json\n",
    "# import io\n",
    "# import time\n",
    "# import logging\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# import resource\n",
    "\n",
    "# site = \"IRWA\"\n",
    "\n",
    "# # True data\n",
    "\n",
    "\n",
    "# data = [\n",
    "#     {\"file\": \"2_2018-05-10_06-39-30\", \"true_herring_counts\": 4},\n",
    "#     {\"file\": \"2_2018-04-14_10-06-19\",\"true_herring_counts\": 1 },\n",
    "#     {\"file\": \"2_2018-04-14_13-18-51\", \"true_herring_counts\": 1},\n",
    "#     {\"file\": \"2_2018-04-28_10-54-38\", \"true_herring_counts\": 3},\n",
    "#     {\"file\": \"2_2017-06-04_06-09-56\", \"true_herring_counts\": 0}, # Comes from left returns right\n",
    "#     {\"file\": \"2_2017-04-15_11-23-36\", \"true_herring_counts\": 1},\n",
    "#     {\"file\": \"2_2017-04-13_14-10-29\", \"true_herring_counts\": 1},  # 2_2017-04-13_13-10-00\n",
    "#     {\"file\": \"2_2017-04-13_13-10-00\", \"true_herring_counts\": 1}, # 2_2018-04-14_17-12-42\n",
    "#     {\"file\": \"2_2018-04-14_17-12-42\", \"true_herring_counts\": 1}, # 2_2018-04-27_13-07-38\n",
    "#     {\"file\": \"2_2018-04-27_13-07-38\", \"true_herring_counts\": 1}, # 2_2018-04-27_15-23-03\n",
    "#     {\"file\": \"2_2018-04-27_15-23-03\", \"true_herring_counts\": 3}, \n",
    "#     {\"file\": \"2_2018-04-29_08-28-10\", \"true_herring_counts\": 1},\n",
    "#     {\"file\": \"2_2018-04-29_09-14-03\", \"true_herring_counts\": 1},\n",
    "#     {\"file\": \"2_2018-04-29_16-28-35\", \"true_herring_counts\": 2},\n",
    "#     {\"file\": \"2_2018-04-29_15-55-24\", \"true_herring_counts\": 2},\n",
    "#     {\"file\": \"2_2018-04-28_11-25-56\", \"true_herring_counts\": 2},\n",
    "#     {\"file\": \"2_2018-04-29_15-39-37\", \"true_herring_counts\": 3},\n",
    "#     {\"file\": \"2_2018-04-29_16-54-05\", \"true_herring_counts\": 1}, \n",
    "#     {\"file\": \"2_2018-05-04_11-32-10\", \"true_herring_counts\": 1}, \n",
    "#     {\"file\": \"2_2018-05-22_05-58-08\", \"true_herring_counts\": 2}, \n",
    "#     {\"file\": \"2_2018-05-04_09-24-42\", \"true_herring_counts\": 2}, # 2_2018-05-04_09-24-42 \n",
    "#     {\"file\": \"2_2018-05-05_10-50-59\", \"true_herring_counts\": 2}, # 2_2018-05-05_10-50-59\n",
    "#     {\"file\": \"2_2018-05-05_13-31-05\", \"true_herring_counts\": 1}, # 2_2018-05-05_13-31-05\n",
    "#     {\"file\": \"2_2018-05-04_18-57-00\", \"true_herring_counts\": 3}, # 2_2018-05-04_18-57-00\n",
    "#     {\"file\": \"2_2018-05-05_13-49-11\", \"true_herring_counts\": 1}, # 2_2018-05-05_13-49-11\n",
    "#     {\"file\": \"2_2018-05-05_18-10-49\", \"true_herring_counts\": 1}, # 2_2018-05-05_18-10-49\n",
    "#     {\"file\": \"2_2018-05-05_18-47-03\", \"true_herring_counts\": 2}, # 2_2018-05-05_18-47-03\n",
    "#     {\"file\": \"2_2018-05-06_08-25-36\", \"true_herring_counts\": 2}, # 2_2018-05-06_08-25-36\n",
    "#     {\"file\": \"2_2018-04-27_12-01-34\", \"true_herring_counts\": 1},#  \n",
    "#     {\"file\": \"2_2018-04-29_18-31-06\", \"true_herring_counts\": 1},# \n",
    "#     {\"file\": \"2_2018-04-29_17-33-32\", \"true_herring_counts\": 2}, # \n",
    "#     {\"file\": \"2_2018-05-02_10-09-38\", \"true_herring_counts\": 2},# \n",
    "#     {\"file\": \"2_2018-05-06_17-32-24\", \"true_herring_counts\": 1}, # 2_2018-05-06_16-35-26\n",
    "#     {\"file\": \"2_2018-05-06_16-35-26\", \"true_herring_counts\": 2}, # \n",
    "#     {\"file\": \"2_2018-05-06_17-08-27\", \"true_herring_counts\": 1}, # \n",
    "#     {\"file\": \"2_2018-05-06_18-09-56\", \"true_herring_counts\": 3},# \n",
    "#     {\"file\": \"2_2018-04-29_16-17-14\", \"true_herring_counts\": 1}, # \n",
    "#     {\"file\": \"2_2018-05-13_11-44-28\", \"true_herring_counts\": 1}, # \n",
    "#     {\"file\": \"2_2018-05-13_07-46-27\", \"true_herring_counts\": 1}, # \n",
    "#     {\"file\": \"2_2018-05-11_19-06-07\", \"true_herring_counts\": 2}, # \n",
    "#     {\"file\": \"2_2018-05-10_17-43-39\", \"true_herring_counts\": 1}, # \n",
    "#     {\"file\": \"2_2018-05-10_17-50-04\", \"true_herring_counts\": 2}, # \n",
    "#     {\"file\": \"2_2018-05-05_13-15-07\", \"true_herring_counts\": 1}, # \n",
    "#     {\"file\": \"2_2018-05-05_13-00-52\", \"true_herring_counts\": 2},#  \n",
    "#     {\"file\": \"2_2018-05-05_13-36-08\", \"true_herring_counts\": 2}, # \n",
    "#     {\"file\": \"2_2018-05-05_12-04-58\", \"true_herring_counts\": 3},# \n",
    "# ]\n",
    "\n",
    "# pred_net_counts = []\n",
    "# true_net_counts = []\n",
    "# total_duration_seconds = []\n",
    "# videos_missed = []\n",
    "# for idx in tqdm(range(len(data))):\n",
    "#     video_path = [path for path in mp4_files if data[idx][\"file\"] in path][0]\n",
    "#     print(video_path)\n",
    "#     frame_rate, annotated_frames, out_count, in_count,duration_seconds,_ = main(\n",
    "#         video_path=video_path,device='mps',stream=False,show=False      \n",
    "#     )\n",
    "#     pred_net_counts.append(out_count-in_count)\n",
    "#     if (out_count-in_count) != data[idx][\"true_herring_counts\"]:\n",
    "#         videos_missed.append({'file':data[idx],'pred':out_count-in_count,'true':data[idx][\"true_herring_counts\"]})\n",
    "#     true_net_counts.append(data[idx][\"true_herring_counts\"])\n",
    "#     total_duration_seconds.append(duration_seconds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "# def calculate_confidence_interval(count_data, confidence_level=0.95):\n",
    "#     # Calculate the mean and standard deviation of the count data\n",
    "#     mean = np.mean(count_data)\n",
    "#     std = np.std(count_data)\n",
    "\n",
    "#     # Calculate the confidence interval using the t-distribution\n",
    "#     n = len(count_data)\n",
    "#     t_value = stats.t.ppf((1 + confidence_level) / 2, df=n-1)\n",
    "#     margin_of_error = t_value * std / np.sqrt(n)\n",
    "#     confidence_interval = (mean - margin_of_error, mean + margin_of_error)\n",
    "\n",
    "#     return confidence_interval\n",
    "\n",
    "# def MAPE(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Calculate the Mean Absolute Percentage Error (MAPE) between the true values and predicted values.\n",
    "    \n",
    "#     Parameters:\n",
    "#         y_true (array-like): Array or list of true values.\n",
    "#         y_pred (array-like): Array or list of predicted values.\n",
    "\n",
    "#     Returns:\n",
    "#         float: The calculated MAPE value.\n",
    "#     \"\"\"\n",
    "#     y_true = np.array(y_true)\n",
    "#     y_pred = np.array(y_pred)\n",
    "\n",
    "#     # Avoid division by zero\n",
    "#     epsilon = 1e-10\n",
    "\n",
    "#     total_percentage_error = 0\n",
    "#     total_samples = len(y_true)\n",
    "\n",
    "#     for i in range(total_samples):\n",
    "#         # Calculate the absolute percentage error for each data point\n",
    "#         if y_true[i] == 0:\n",
    "#             absolute_percentage_error = np.abs((y_true[i] - y_pred[i]))\n",
    "#         else:\n",
    "#             absolute_percentage_error = np.abs((y_true[i] - y_pred[i]) / (y_true[i] + epsilon))\n",
    "\n",
    "#         # Add it to the total percentage error\n",
    "#         total_percentage_error += absolute_percentage_error\n",
    "\n",
    "#     # Calculate the mean of the absolute percentage errors\n",
    "#     mape = (total_percentage_error / total_samples) * 100.0\n",
    "\n",
    "#     return mape\n",
    "\n",
    "# df_results = pd.DataFrame(data={\"pred_net_counts\":pred_net_counts,\"true_net_counts\":true_net_counts},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"num videos predicted wrong: \",len(videos_missed))\n",
    "# print(f\"Total video processed: {sum(total_duration_seconds)} (sec), {sum(total_duration_seconds)/60:.1f} (min)\")\n",
    "# print(f\"Total number of videos process: {len(data)}\")\n",
    "\n",
    "# diffs = [np.abs(x - y) for x, y in zip(true_net_counts, pred_net_counts)]\n",
    "# print(f\"Did not count {sum(diffs)} out of {sum(true_net_counts)} herring\")\n",
    "# print(f\"MAE (Mean Absolute Error e.g. counts): {mean_absolute_error(y_pred=pred_net_counts,y_true = true_net_counts):.2f}\",)\n",
    "# print(f\"MAPE (Mean Absolute Percentage Error of Counts): {MAPE(true_net_counts,pred_net_counts):.1f}%\",)\n",
    "\n",
    "# print(f\"Average number of missed counts per second {sum(diffs)/sum(total_duration_seconds):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# absolute_error = np.abs(df_results[\"pred_net_counts\"] - df_results[\"true_net_counts\"]).values\n",
    "# std_absolute_error = np.std(absolute_error)\n",
    "# ci = calculate_confidence_interval(count_data=diffs)\n",
    "# print(f\"Estimates off between {ci[0]:.1f} and {ci[1]:.1f} fish at an average of {np.mean(diffs):.1f} per video\")\n",
    "\n",
    "# # Bootstrapped\n",
    "\n",
    "# def bootstrap_mean_ci(data, num_iterations=10000, ci_level=0.95):\n",
    "#     # Create an array to store bootstrapped means\n",
    "#     bootstrapped_means = np.empty(num_iterations)\n",
    "\n",
    "#     # Perform bootstrapping\n",
    "#     for i in range(num_iterations):\n",
    "#         bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "#         bootstrapped_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "#     # Calculate confidence interval\n",
    "#     lower_ci = np.percentile(bootstrapped_means, (1 - ci_level) * 100 / 2)\n",
    "#     upper_ci = np.percentile(bootstrapped_means, ci_level * 100 + (1 - ci_level) * 100 / 2)\n",
    "\n",
    "#     return np.mean(bootstrapped_means), lower_ci, upper_ci\n",
    "\n",
    "# # # Example usage\n",
    "# # mean, lower_ci, upper_ci = bootstrap_mean_ci(diffs)\n",
    "\n",
    "# # print(f\"Bootstrapped mean: {mean:.2f}\")\n",
    "# # print(f\"Bootstrapped CI of how many counts on average the model is off: [{lower_ci:.2f}, {upper_ci:.2f}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num videos predicted wrong:  8\n",
    "Total video processed: 417.93 (sec), 7.0 (min)\n",
    "Total number of videos process: 32\n",
    "Did not count 10 out of 52 herring\n",
    "MAE (Mean Absolute Error e.g. counts): 0.31\n",
    "MAPE (Mean Absolute Percentage Error of Counts): 19.8%\n",
    "Average number of missed counts per second 0.02\n",
    "Estimates off between 0.1 and 0.5 fish at an average of 0.3 per vide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    " \"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/runs/colab_runs/last3.pt\" # best12.pt\n",
    "num videos predicted wrong:  9\n",
    "Total video processed: 598.044 (sec), 10.0 (min)\n",
    "Total number of videos process: 46\n",
    "Did not count 11 out of 75 herring\n",
    "MAE (Mean Absolute Error e.g. counts): 0.24\n",
    "MAPE (Mean Absolute Percentage Error of Counts): 11.6%\n",
    "Average number of missed counts per second 0.02\n",
    "Estimates off between 0.1 and 0.4 fish at an average of 0.2 per video\n",
    "\n",
    "\n",
    "\"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/runs/colab_runs/last2.pt\" (medium model)\n",
    "num videos predicted wrong:  9\n",
    "Total video processed: 598.044 (sec), 10.0 (min)\n",
    "Total number of videos process: 46\n",
    "Did not count 11 out of 75 herring\n",
    "MAE (Mean Absolute Error e.g. counts): 0.24\n",
    "MAPE (Mean Absolute Percentage Error of Counts): 13.0%\n",
    "Average number of missed counts per second 0.02\n",
    "Estimates off between 0.1 and 0.4 fish at an average of 0.2 per video\n",
    "\n",
    "\"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/runs/colab_runs/best5.pt\"\n",
    "Total video processed: 370.959 (sec), 6.2 (min)\n",
    "Total number of videos process: 28\n",
    "Did not count 8 out of 46 herring\n",
    "MAE (Mean Absolute Error e.g. counts): 0.29\n",
    "MAPE (Mean Absolute Percentage Error of Counts): 15.5%\n",
    "Average number of missed counts per second 0.02\n",
    "Estimates off between 0.1 and 0.5 fish at an average of 0.3 per video\n",
    "\n",
    "\n",
    "\"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/runs/colab_runs/best3.pt\"\n",
    "\n",
    "Total video processed: 347.549 (sec), 5.8 (min)\n",
    "Total number of videos process: 26\n",
    "Did not count 7 out of 42 herring\n",
    "MAE (Mean Absolute Error e.g. counts): 0.27\n",
    "MAPE (Mean Absolute Percentage Error of Counts): 16.7%\n",
    "Average number of missed counts per second 0.02\n",
    "Estimates off between 0.0 and 0.5 fish at an average of 0.3 per video\n",
    "\n",
    "\n",
    " \"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/runs/colab_runs/best2.pt\"\n",
    " \n",
    "Total video processed: 311.58 (sec), 5.2 (min)\n",
    "Total number of videos process: 23\n",
    "Did not count 6 out of 37 herring\n",
    "MAE (Mean Absolute Error e.g. counts): 0.26\n",
    "MAPE (Mean Absolute Percentage Error of Counts): 17.4%\n",
    "Average number of missed counts per second 0.02\n",
    "Estimates off between -0.0 and 0.5 fish at an average of 0.3 per video\n",
    "\n",
    "\n",
    "/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/runs/detect/train79/weights/last.pt\n",
    "\n",
    "Total video processed: 311.58 (sec), 5.2 (min)\n",
    "Total number of videos process: 23\n",
    "Did not count 10 out of 37 herring\n",
    "MAE (Mean Absolute Error e.g. counts): 0.43\n",
    "MAPE (Mean Absolute Percentage Error of Counts): 21.0%\n",
    "Average number of missed counts per second 0.03\n",
    "Estimates off between 0.2 and 0.7 fish at an average of 0.4 per video\n",
    "\n",
    "\n",
    "# /Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/runs/detect/train196/weights/last.pt\n",
    "\n",
    "Total video processed: 284.347 (sec), 4.7 (min)\n",
    "Total number of videos process: 21\n",
    "Did not count 14 out of 34 herring\n",
    "MAE (Mean Absolute Error e.g. counts): 0.67\n",
    "MAPE (Mean Absolute Percentage Error of Counts): 38.1%\n",
    "Average number of missed counts per second 0.05\n",
    "Estimates off between 0.3 and 1.0 fish at an average of 0.7 per video\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate True Counts using Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "\n",
    "# Simulated data (detections in each video)\n",
    "observed_data =  [s[\"true_herring_counts\"] for s in data]#np.array([4, 1, 1, 3, 0, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 2, 3, 1, 1, 2, 2, 2, 1, 3, 1])\n",
    "\n",
    "# Number of videos\n",
    "n_videos = len(observed_data)\n",
    "\n",
    "# Prior belief (Poisson with mean 1)\n",
    "prior_lambda = 1\n",
    "\n",
    "# Model definition\n",
    "with pm.Model() as model:\n",
    "    # Prior distribution\n",
    "    theta = pm.Poisson('theta', mu=prior_lambda)\n",
    "    \n",
    "    # Likelihood (Binomial)\n",
    "    likelihood = pm.Binomial('likelihood', n=n_videos, p=theta/n_videos, observed=observed_data)\n",
    "    \n",
    "    # Sample from the posterior using MCMC\n",
    "    trace = pm.sample(10000, tune=1000, cores=2)  # Adjust tune and cores as needed\n",
    "\n",
    "# Plot posterior distribution\n",
    "pm.plot_posterior(trace, var_names=['theta'])\n",
    "\n",
    "# Print posterior summary\n",
    "print(pm.summary(trace, var_names=['theta']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "\n",
    "# Given values\n",
    "p = 0.88\n",
    "observed_data#np.array([4.0, 1, 1, 3, 0, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 2, 3, 1, 1, 2, 2, 2, 1, 3, 1])\n",
    "n = len(observed_data)\n",
    "\n",
    "# Example value of theta\n",
    "theta = 1.9\n",
    "\n",
    "# Calculate the likelihood\n",
    "likelihood = comb(n, observed_data) * (p ** observed_data) * ((1 - p) ** (n - observed_data))\n",
    "likelihood_for_theta = likelihood.prod()\n",
    "\n",
    "#print(\"Likelihood for theta =\", theta, \":\", likelihood_for_theta)\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p ** observed_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is calculation for Z\n",
    "\n",
    "Posterior(Î¸âˆ£data)= \n",
    "1/Z\n",
    "â€‹\n",
    " Ã—Prior(Î¸)Ã—Likelihood(dataâˆ£Î¸)\n",
    "\n",
    "\n",
    "The likelihood function represents the probability of observing the given data (observed fish detection counts) given a specific value of the parameter \n",
    "ï¿½\n",
    "Î¸. In your case, the likelihood is modeled as a Binomial distribution because you're dealing with counts of successes (fish detections) in a fixed number of trials (videos). The likelihood function is given by:\n",
    "\n",
    "Likelihood\n",
    "(\n",
    "data\n",
    "â€‰\n",
    "âˆ£\n",
    "â€‰\n",
    "ï¿½\n",
    ")\n",
    "=\n",
    "Binomial\n",
    "(\n",
    "data\n",
    "â€‰\n",
    "âˆ£\n",
    "â€‰\n",
    "ï¿½\n",
    ",\n",
    "ï¿½\n",
    ")\n",
    "Likelihood(dataâˆ£Î¸)=Binomial(dataâˆ£n,p)\n",
    "\n",
    "Where:\n",
    "\n",
    "data\n",
    "data is the observed fish detection counts.\n",
    "ï¿½\n",
    "n is the number of trials (number of videos in this case).\n",
    "ï¿½\n",
    "p is the probability of success (probability of detecting a fish in a video), which is \n",
    "ï¿½\n",
    "ï¿½\n",
    "n\n",
    "Î¸\n",
    "â€‹\n",
    " .\n",
    "Given your observed data and a specific value of \n",
    "ï¿½\n",
    "Î¸, you can plug in the values into the Binomial distribution formula to calculate the likelihood of observing the data:\n",
    "\n",
    "Likelihood\n",
    "(\n",
    "data\n",
    "â€‰\n",
    "âˆ£\n",
    "â€‰\n",
    "ï¿½\n",
    ")\n",
    "=\n",
    "(\n",
    "ï¿½\n",
    "data\n",
    ")\n",
    "â‹…\n",
    "ï¿½\n",
    "data\n",
    "â‹…\n",
    "(\n",
    "1\n",
    "âˆ’\n",
    "ï¿½\n",
    ")\n",
    "ï¿½\n",
    "âˆ’\n",
    "data\n",
    "Likelihood(dataâˆ£Î¸)=( \n",
    "data\n",
    "n\n",
    "â€‹\n",
    " )â‹…p \n",
    "data\n",
    " â‹…(1âˆ’p) \n",
    "nâˆ’data\n",
    " \n",
    "\n",
    "In your example, you can calculate the likelihood for each observed fish detection count using the formula above and the corresponding value of \n",
    "ï¿½\n",
    "Î¸."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.benchmarks import benchmark\n",
    "benchmark(model='/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/code/notebooks/runs/detect/train184/weights/best.pt', data='river_herring.yaml', imgsz=(320,240), half=False, device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted datetime: 2018-04-14 10:06:19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.1\n",
       "2       0.2\n",
       "3       0.3\n",
       "4       0.4\n",
       "       ... \n",
       "132    13.2\n",
       "133    13.3\n",
       "134    13.4\n",
       "135    13.5\n",
       "136    13.6\n",
       "Name: Relative Time, Length: 137, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from datetime import timedelta\n",
    "\n",
    "def extract_datetime_from_filename(filename):\n",
    "  \"\"\"\n",
    "  Extracts date and time from a filename with specific format.\n",
    "\n",
    "  Args:\n",
    "      filename (str): The filename to extract datetime from.\n",
    "\n",
    "  Returns:\n",
    "      datetime.datetime: The extracted datetime object or None if not found.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If the filename format doesn't match the expected pattern.\n",
    "  \"\"\"\n",
    "  pattern = r\"(\\d+)_(\\d{4}-\\d{2}-\\d{2})_(\\d{2}-\\d{2}-\\d{2})\\.(.+)?\"\n",
    "  match = re.search(pattern, filename)\n",
    "\n",
    "  if match:\n",
    "    # Extract captured groups\n",
    "    _, date_str, time_str, _ = match.groups()\n",
    "    # Combine date and time strings\n",
    "    datetime_str = f\"{date_str} {time_str}\"\n",
    "    # Try converting to datetime object\n",
    "    try:\n",
    "      from datetime import datetime\n",
    "      return datetime.strptime(datetime_str, \"%Y-%m-%d %H-%M-%S\")\n",
    "    except ValueError:\n",
    "      raise ValueError(\"Invalid date or time format in filename\")\n",
    "  else:\n",
    "    raise ValueError(\"Filename format doesn't match expected pattern\")\n",
    "\n",
    "\n",
    "try:\n",
    "  extracted_datetime = extract_datetime_from_filename(filename)\n",
    "  print(f\"Extracted datetime: {extracted_datetime}\")\n",
    "except ValueError as e:\n",
    "  print(f\"Error: {e}\")\n",
    "\n",
    "def create_timestamps(relative_frame_times, reference_datetime):\n",
    "  \"\"\"\n",
    "  Creates a list of timestamps from relative frame times and a reference datetime.\n",
    "\n",
    "  Args:\n",
    "      relative_frame_times (list): A list of frame times in seconds (floats).\n",
    "      reference_datetime (datetime.datetime): The reference datetime for the video.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of datetime objects corresponding to each frame time.\n",
    "  \"\"\"\n",
    "  timestamps = []\n",
    "  for frame_time in relative_frame_times:\n",
    "    # Convert frame time to timedelta\n",
    "    time_delta = timedelta(seconds=frame_time)\n",
    "    # Add timedelta to reference datetime to get timestamp\n",
    "    timestamp = reference_datetime + time_delta\n",
    "    formatted_timestamp = timestamp.strftime(timestamp)\n",
    "    timestamps.append(formatted_timestamp)\n",
    "  return timestamps\n",
    "\n",
    "\n",
    "#invalide_filename = \"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/data/gold_dataset/videos/invalid_name.mp4\"\n",
    "valid_filename = \"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/data/gold_dataset/videos/2_2018-04-14_10-06-19.mp4\"\n",
    "# Example usage\n",
    "filename = \"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/data/gold_dataset/videos/2_2018-04-14_10-06-19.mp4\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/aus10powell/Downloads/2_2018-04-14_10-06-19_annotated_detections.csv\")\n",
    "\n",
    "\n",
    "# for fname  in [invalide_filename,valid_filename]:\n",
    "#     f = extract_datetime_from_filename(filename=fname)\n",
    "#     print(f)\n",
    "\n",
    "#create_timestamps(df[\"Relative Time\"],reference_datetime=f)\n",
    "# f + timedelta(seconds=17)\n",
    "df[\"Relative Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-04-14 10:06:19.000000',\n",
       " '2018-04-14 10:06:19.100000',\n",
       " '2018-04-14 10:06:19.200000',\n",
       " '2018-04-14 10:06:19.300000',\n",
       " '2018-04-14 10:06:19.400000',\n",
       " '2018-04-14 10:06:19.500000',\n",
       " '2018-04-14 10:06:19.600000',\n",
       " '2018-04-14 10:06:19.700000',\n",
       " '2018-04-14 10:06:19.800000',\n",
       " '2018-04-14 10:06:19.900000',\n",
       " '2018-04-14 10:06:20.000000',\n",
       " '2018-04-14 10:06:20.100000',\n",
       " '2018-04-14 10:06:20.200000',\n",
       " '2018-04-14 10:06:20.300000',\n",
       " '2018-04-14 10:06:20.400000',\n",
       " '2018-04-14 10:06:20.500000',\n",
       " '2018-04-14 10:06:20.600000',\n",
       " '2018-04-14 10:06:20.700000',\n",
       " '2018-04-14 10:06:20.800000',\n",
       " '2018-04-14 10:06:20.900000',\n",
       " '2018-04-14 10:06:21.000000',\n",
       " '2018-04-14 10:06:21.100000',\n",
       " '2018-04-14 10:06:21.200000',\n",
       " '2018-04-14 10:06:21.300000',\n",
       " '2018-04-14 10:06:21.400000',\n",
       " '2018-04-14 10:06:21.500000',\n",
       " '2018-04-14 10:06:21.600000',\n",
       " '2018-04-14 10:06:21.700000',\n",
       " '2018-04-14 10:06:21.800000',\n",
       " '2018-04-14 10:06:21.900000',\n",
       " '2018-04-14 10:06:22.000000',\n",
       " '2018-04-14 10:06:22.100000',\n",
       " '2018-04-14 10:06:22.200000',\n",
       " '2018-04-14 10:06:22.300000',\n",
       " '2018-04-14 10:06:22.400000',\n",
       " '2018-04-14 10:06:22.500000',\n",
       " '2018-04-14 10:06:22.600000',\n",
       " '2018-04-14 10:06:22.700000',\n",
       " '2018-04-14 10:06:22.800000',\n",
       " '2018-04-14 10:06:22.900000',\n",
       " '2018-04-14 10:06:23.000000',\n",
       " '2018-04-14 10:06:23.100000',\n",
       " '2018-04-14 10:06:23.200000',\n",
       " '2018-04-14 10:06:23.300000',\n",
       " '2018-04-14 10:06:23.400000',\n",
       " '2018-04-14 10:06:23.500000',\n",
       " '2018-04-14 10:06:23.600000',\n",
       " '2018-04-14 10:06:23.700000',\n",
       " '2018-04-14 10:06:23.800000',\n",
       " '2018-04-14 10:06:23.900000',\n",
       " '2018-04-14 10:06:24.000000',\n",
       " '2018-04-14 10:06:24.100000',\n",
       " '2018-04-14 10:06:24.200000',\n",
       " '2018-04-14 10:06:24.300000',\n",
       " '2018-04-14 10:06:24.400000',\n",
       " '2018-04-14 10:06:24.500000',\n",
       " '2018-04-14 10:06:24.600000',\n",
       " '2018-04-14 10:06:24.700000',\n",
       " '2018-04-14 10:06:24.800000',\n",
       " '2018-04-14 10:06:24.900000',\n",
       " '2018-04-14 10:06:25.000000',\n",
       " '2018-04-14 10:06:25.100000',\n",
       " '2018-04-14 10:06:25.200000',\n",
       " '2018-04-14 10:06:25.300000',\n",
       " '2018-04-14 10:06:25.400000',\n",
       " '2018-04-14 10:06:25.500000',\n",
       " '2018-04-14 10:06:25.600000',\n",
       " '2018-04-14 10:06:25.700000',\n",
       " '2018-04-14 10:06:25.800000',\n",
       " '2018-04-14 10:06:25.900000',\n",
       " '2018-04-14 10:06:26.000000',\n",
       " '2018-04-14 10:06:26.100000',\n",
       " '2018-04-14 10:06:26.200000',\n",
       " '2018-04-14 10:06:26.300000',\n",
       " '2018-04-14 10:06:26.400000',\n",
       " '2018-04-14 10:06:26.500000',\n",
       " '2018-04-14 10:06:26.600000',\n",
       " '2018-04-14 10:06:26.700000',\n",
       " '2018-04-14 10:06:26.800000',\n",
       " '2018-04-14 10:06:26.900000',\n",
       " '2018-04-14 10:06:27.000000',\n",
       " '2018-04-14 10:06:27.100000',\n",
       " '2018-04-14 10:06:27.200000',\n",
       " '2018-04-14 10:06:27.300000',\n",
       " '2018-04-14 10:06:27.400000',\n",
       " '2018-04-14 10:06:27.500000',\n",
       " '2018-04-14 10:06:27.600000',\n",
       " '2018-04-14 10:06:27.700000',\n",
       " '2018-04-14 10:06:27.800000',\n",
       " '2018-04-14 10:06:27.900000',\n",
       " '2018-04-14 10:06:28.000000',\n",
       " '2018-04-14 10:06:28.100000',\n",
       " '2018-04-14 10:06:28.200000',\n",
       " '2018-04-14 10:06:28.300000',\n",
       " '2018-04-14 10:06:28.400000',\n",
       " '2018-04-14 10:06:28.500000',\n",
       " '2018-04-14 10:06:28.600000',\n",
       " '2018-04-14 10:06:28.700000',\n",
       " '2018-04-14 10:06:28.800000',\n",
       " '2018-04-14 10:06:28.900000',\n",
       " '2018-04-14 10:06:29.000000',\n",
       " '2018-04-14 10:06:29.100000',\n",
       " '2018-04-14 10:06:29.200000',\n",
       " '2018-04-14 10:06:29.300000',\n",
       " '2018-04-14 10:06:29.400000',\n",
       " '2018-04-14 10:06:29.500000',\n",
       " '2018-04-14 10:06:29.600000',\n",
       " '2018-04-14 10:06:29.700000',\n",
       " '2018-04-14 10:06:29.800000',\n",
       " '2018-04-14 10:06:29.900000',\n",
       " '2018-04-14 10:06:30.000000',\n",
       " '2018-04-14 10:06:30.100000',\n",
       " '2018-04-14 10:06:30.200000',\n",
       " '2018-04-14 10:06:30.300000',\n",
       " '2018-04-14 10:06:30.400000',\n",
       " '2018-04-14 10:06:30.500000',\n",
       " '2018-04-14 10:06:30.600000',\n",
       " '2018-04-14 10:06:30.700000',\n",
       " '2018-04-14 10:06:30.800000',\n",
       " '2018-04-14 10:06:30.900000',\n",
       " '2018-04-14 10:06:31.000000',\n",
       " '2018-04-14 10:06:31.100000',\n",
       " '2018-04-14 10:06:31.200000',\n",
       " '2018-04-14 10:06:31.300000',\n",
       " '2018-04-14 10:06:31.400000',\n",
       " '2018-04-14 10:06:31.500000',\n",
       " '2018-04-14 10:06:31.600000',\n",
       " '2018-04-14 10:06:31.700000',\n",
       " '2018-04-14 10:06:31.800000',\n",
       " '2018-04-14 10:06:31.900000',\n",
       " '2018-04-14 10:06:32.000000',\n",
       " '2018-04-14 10:06:32.100000',\n",
       " '2018-04-14 10:06:32.200000',\n",
       " '2018-04-14 10:06:32.300000',\n",
       " '2018-04-14 10:06:32.400000',\n",
       " '2018-04-14 10:06:32.500000',\n",
       " '2018-04-14 10:06:32.600000']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "valid_filename = \"/Users/aus10powell/Documents/Projects/MIT-Fishery-Counter/data/gold_dataset/videos/2_2018-04-14_10-06-19.mp4\"\n",
    "\n",
    "# Extract reference datetime (assuming this logic exists elsewhere)\n",
    "reference_datetime = extract_datetime_from_filename(valid_filename)\n",
    "\n",
    "# Read your CSV file\n",
    "df = pd.read_csv(\"/Users/aus10powell/Downloads/2_2018-04-14_10-06-19_annotated_detections.csv\")\n",
    "\n",
    "# Create formatted timestamps based on reference datetime and relative times\n",
    "formatted_timestamps = create_timestamps(df[\"Relative Time\"], reference_datetime)\n",
    "\n",
    "# Now you have a list of formatted timestamps (formatted_timestamps)\n",
    "# You can add this as a new column to your DataFrame or use it for further analysis\n",
    "formatted_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_processesor_type():\n",
    "    \"\"\"\n",
    "    Get the type of processor available on the system.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        return \"gpu\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "if get_processesor_type()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
